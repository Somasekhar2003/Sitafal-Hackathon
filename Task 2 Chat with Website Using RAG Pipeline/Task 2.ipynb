{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.25.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.11.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.12 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.12)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.2.3)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.7.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (0.27.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (3.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (0.21.0)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (4.47.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-ollama) (0.4.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (0.3.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (2.27.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91949\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community langchain-huggingface requests beautifulsoup4 langchain-ollama pymupdf faiss-cpu python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 14\n",
      "Documents added: 14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import faiss\n",
    "\n",
    "# Load environment variables and suppress warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "# Function to scrape website data\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract meaningful text (e.g., skip headers, menus)\n",
    "        main_content = soup.find('main')  # Most modern sites use <main> for content\n",
    "        if main_content:\n",
    "            return main_content.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            return soup.get_text(separator=\"\\n\", strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# List of target websites\n",
    "websites = [\n",
    "    \"https://www.washington.edu/\",\n",
    "    \"https://www.stanford.edu/\",\n",
    "    \"https://und.edu/\"\n",
    "]\n",
    "\n",
    "# Scrape each website and store content with metadata\n",
    "website_contents = []\n",
    "for website in websites:\n",
    "    content = scrape_website(website)\n",
    "    if content:\n",
    "        website_contents.append({\"content\": content, \"metadata\": {\"url\": website}})\n",
    "\n",
    "# Split content into chunks and convert to Document objects\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "document_chunks = []\n",
    "for item in website_contents:\n",
    "    raw_chunks = text_splitter.split_text(item[\"content\"])\n",
    "    document_chunks.extend([\n",
    "        Document(page_content=chunk, metadata={\"url\": item[\"metadata\"][\"url\"]})\n",
    "        for chunk in raw_chunks if len(chunk.strip()) > 50  # Skip overly short chunks\n",
    "    ])\n",
    "\n",
    "\n",
    "print(f\"Number of chunks created: {len(document_chunks)}\")\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url=\"http://localhost:11434\")\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=faiss.IndexFlatL2(len(embeddings.embed_query(\"test\"))),\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "ids = vector_store.add_documents(documents=document_chunks)\n",
    "print(f\"Documents added: {len(ids)}\")\n",
    "\n",
    "# Save the vector store\n",
    "db_name = \"website_embeddings\"\n",
    "vector_store.save_local(db_name)\n",
    "\n",
    "# Load the vector store for querying\n",
    "new_vector_store = FAISS.load_local(db_name, embeddings=embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: https://www.stanford.edu/\n",
      "Main Content\n",
      "A Societal Mission\n",
      "Stanford was founded almost 150 years ago on a bedrock of societal purpose. Our mission is to contribute to the world by educating students for lives of leadership and contribution with integrity; advancing fundamental knowledge and cultivating creativity; leading in pioneering research for effective clinical therapies; and accelerating solutions and amplifying their impact.\n",
      "More about Stanford\n",
      "Campus News\n",
      "Stories about people, research, and innovation across the Farm\n",
      "Science & Engineering\n",
      "Stanford welcomes first GPU-based supercomputer\n",
      "Health & Medicine\n",
      "Flu virus remains infectious in refrigerated raw milk for up to five days, new study shows\n",
      "Science & Engineering\n",
      "A new report warns of serious risks from ‘mirror life’\n",
      "Awards\n",
      "Five from Stanford named Marshall Scholars\n",
      "Science & Engineering\n",
      "New device produces critical fertilizer ingredient from thin air\n",
      "Science & Engineering\n",
      "Scientists call for all-out, global effort to create an AI virtual cell\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the history of Stanford University?\"\n",
    "docs = new_vector_store.search(query=question, search_type='similarity')\n",
    "\n",
    "# Post-process to remove unrelated chunks\n",
    "filtered_docs = [\n",
    "    doc for doc in docs if \"history\" in doc.page_content.lower() or \"founded\" in doc.page_content.lower()\n",
    "]\n",
    "\n",
    "for doc in filtered_docs:\n",
    "    print(f\"Source: {doc.metadata['url']}\")\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 50\n",
      "Documents added: 50\n",
      "Results for Query: When was Stanford University founded?\n",
      "Source: https://www.stanford.edu/about/\n",
      "Stanford was founded in 1885 by California senator Leland Stanford and his wife, Jane, “to promote the public welfare by exercising an influence in behalf of humanity and civilization.” The university is governed by a Board of Trustees, President, Provost, Academic Council and a number of other\n",
      "\n",
      "\n",
      "Results for Query: What research is done at Stanford?\n",
      "Source: https://www.stanford.edu/about/\n",
      "university’s mission, and students have extensive opportunities to join Stanford scholars in research that develops new knowledge and deepens understanding of ourselves and the world around us. A hallmark of Stanford is our extensive and vibrant ecosystem of interdisciplinary research. With all\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import faiss\n",
    "\n",
    "# Load environment variables and suppress warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "# Function to scrape website data\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract main content only\n",
    "        main_content = soup.find('main')  # Most modern sites use <main> for content\n",
    "        if main_content:\n",
    "            return main_content.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            return soup.get_text(separator=\"\\n\", strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Target website (Stanford University's \"About\" page)\n",
    "website = \"https://www.stanford.edu/about/\"\n",
    "\n",
    "# Scrape the website\n",
    "content = scrape_website(website)\n",
    "website_contents = [{\"content\": content, \"metadata\": {\"url\": website}}]\n",
    "\n",
    "# Split content into chunks and convert to Document objects\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)  # Smaller chunks for specificity\n",
    "document_chunks = []\n",
    "for item in website_contents:\n",
    "    raw_chunks = text_splitter.split_text(item[\"content\"])\n",
    "    document_chunks.extend([\n",
    "        Document(page_content=chunk, metadata={\"url\": item[\"metadata\"][\"url\"]})\n",
    "        for chunk in raw_chunks\n",
    "    ])\n",
    "\n",
    "print(f\"Number of chunks created: {len(document_chunks)}\")\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=faiss.IndexFlatL2(len(embeddings.embed_query(\"test\"))),\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "ids = vector_store.add_documents(documents=document_chunks)\n",
    "print(f\"Documents added: {len(ids)}\")\n",
    "\n",
    "# Save the vector store\n",
    "db_name = \"website_embeddings\"\n",
    "vector_store.save_local(db_name)\n",
    "\n",
    "# Load the vector store for querying\n",
    "new_vector_store = FAISS.load_local(db_name, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Query the vector store\n",
    "questions = [\n",
    "    \"When was Stanford University founded?\",\n",
    "    \"What research is done at Stanford?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    # Search for the question in the vector store\n",
    "    docs = new_vector_store.search(query=question, search_type='similarity', k=1)  # Retrieve more results for better filtering\n",
    "    \n",
    "    # Filter documents based on relevance\n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "        # Check if the chunk is relevant to the query\n",
    "        if any(keyword.lower() in doc.page_content.lower() for keyword in question.split()):\n",
    "            filtered_docs.append(doc)\n",
    "    \n",
    "    # Select only the most relevant document for the query\n",
    "    if filtered_docs:\n",
    "        best_doc = max(filtered_docs, key=lambda doc: len(doc.page_content.split()))\n",
    "        print(f\"Results for Query: {question}\")\n",
    "        print(f\"Source: {best_doc.metadata['url']}\")\n",
    "        print(best_doc.page_content)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No relevant information found for query: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 50\n",
      "Documents added: 50\n",
      "Enter questions about Stanford University ('exit' to quit):\n",
      "Results for Query: What is Stanford University's mission?\n",
      "Source: https://www.stanford.edu/about/\n",
      "Stanford was founded almost 150 years ago on a bedrock of societal purpose. Our mission is to contribute to the world by educating students for lives of leadership and contribution with integrity; advancing fundamental knowledge and cultivating creativity; leading in pioneering research for\n",
      "\n",
      "\n",
      "Results for Query: What research is done at Stanford?\n",
      "Source: https://www.stanford.edu/about/\n",
      "university’s mission, and students have extensive opportunities to join Stanford scholars in research that develops new knowledge and deepens understanding of ourselves and the world around us. A hallmark of Stanford is our extensive and vibrant ecosystem of interdisciplinary research. With all\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import faiss\n",
    "\n",
    "# Load environment variables and suppress warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "# Function to scrape website data\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract main content only\n",
    "        main_content = soup.find('main')  # Most modern sites use <main> for content\n",
    "        if main_content:\n",
    "            return main_content.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            return soup.get_text(separator=\"\\n\", strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Target website (Stanford University's \"About\" page)\n",
    "website = \"https://www.stanford.edu/about/\"\n",
    "\n",
    "# Scrape the website\n",
    "content = scrape_website(website)\n",
    "website_contents = [{\"content\": content, \"metadata\": {\"url\": website}}]\n",
    "\n",
    "# Split content into chunks and convert to Document objects\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)  # Smaller chunks for specificity\n",
    "document_chunks = []\n",
    "for item in website_contents:\n",
    "    raw_chunks = text_splitter.split_text(item[\"content\"])\n",
    "    document_chunks.extend([\n",
    "        Document(page_content=chunk, metadata={\"url\": item[\"metadata\"][\"url\"]})\n",
    "        for chunk in raw_chunks\n",
    "    ])\n",
    "\n",
    "print(f\"Number of chunks created: {len(document_chunks)}\")\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=faiss.IndexFlatL2(len(embeddings.embed_query(\"test\"))),\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "ids = vector_store.add_documents(documents=document_chunks)\n",
    "print(f\"Documents added: {len(ids)}\")\n",
    "\n",
    "# Save the vector store\n",
    "db_name = \"website_embeddings\"\n",
    "vector_store.save_local(db_name)\n",
    "\n",
    "# Load the vector store for querying\n",
    "new_vector_store = FAISS.load_local(db_name, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Query the vector store with dynamic user input\n",
    "print(\"Enter questions about Stanford University ('exit' to quit):\")\n",
    "while True:\n",
    "    question = input(\"> \").strip()\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    # Search for the question in the vector store\n",
    "    docs = new_vector_store.search(query=question, search_type='similarity', k=1)  # Retrieve more results for better filtering\n",
    "    \n",
    "    # Filter documents based on relevance\n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "        # Check if the chunk is relevant to the query\n",
    "        if any(keyword.lower() in doc.page_content.lower() for keyword in question.split()):\n",
    "            filtered_docs.append(doc)\n",
    "    \n",
    "    # If there are relevant documents, select the top ones\n",
    "    if filtered_docs:\n",
    "        best_doc = max(filtered_docs, key=lambda doc: len(doc.page_content.split()))\n",
    "        print(f\"Results for Query: {question}\")\n",
    "        print(f\"Source: {best_doc.metadata['url']}\")\n",
    "        print(best_doc.page_content)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No relevant information found for query: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"What is Stanford University's mission?\",\n",
    "#\"When was Stanford University founded?\",\n",
    "#\"What are Stanford's academic programs?\",\n",
    "#\"What research is done at Stanford?\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
